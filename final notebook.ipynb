{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zyfra Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this project is to prepare a prototype of a machine learning model for Zyfra, a company that develops efficiency solutions for the heavy industry. The model will aim to predict the amount of gold recovered from gold ore. The features we will use will be data on gold extraction and purification. The goal is to have the model optimize the production and eliminate unprofitable parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly_express"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import useful libraries\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from scipy import stats as st\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error as mse, r2_score, mean_absolute_percentage_error as mape, mean_absolute_error as mae , make_scorer\n",
    "import plotly_express as px\n",
    "import plotly.graph_objects as go \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataframes\n",
    "full = pd.read_csv('/datasets/gold_recovery_full.csv')\n",
    "test = pd.read_csv('/datasets/gold_recovery_test.csv')\n",
    "train = pd.read_csv('/datasets/gold_recovery_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of the data\n",
    "print(full.shape)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test data has fewer columns than the other datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking for missing values\n",
    "print(full.isna().sum())\n",
    "print()\n",
    "print(train.isna().sum())\n",
    "print()\n",
    "print(test.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see missing values in the various datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating train recovery list\n",
    "train = train.dropna()\n",
    "train_recovery = train['rougher.output.recovery'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values in train recovery\n",
    "train_recovery.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of dataframe\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recovery calculation\n",
    "\n",
    "data = train.dropna()\n",
    "\n",
    "c = data['rougher.output.concentrate_au']     # share of gold in concentrate after flotation\n",
    "f = data['rougher.input.feed_au']             # share of gold in feed before flotation\n",
    "t = data['rougher.output.tail_au']  # share of gold in the rougher tails after flotation\n",
    "\n",
    "calc_recovery = (c * (f-t)) / (f * (c-t)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values in calculated recovery\n",
    "calc_recovery.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of dataframe to see if it matches\n",
    "calc_recovery.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating recovery difference between dat and calculation\n",
    "recovery_difference = train_recovery - calc_recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recovery difference metrics\n",
    "recovery_difference.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating recovery merged dataframe\n",
    "recovery_merged = pd.concat([train_recovery.reset_index(drop=True), calc_recovery.reset_index(drop=True)], axis=1)\n",
    "recovery_merged.columns = ['train', 'calc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "recovery_merged[recovery_merged.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total of missing values\n",
    "recovery_merged.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop missing values\n",
    "recovery_merged.dropna(how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE score between train and calculated \n",
    "print(mae(recovery_merged.train, recovery_merged.calc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean absolute error between the recovery value in the dataset, and the calculated recovery, is 9.46 x 10^-15. The difference between this two values is on average, unnoticeable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at column names among the different datasets\n",
    "test_cols = test.columns\n",
    "full_cols = full.columns\n",
    "train_cols = train.columns\n",
    "\n",
    "full_not_test = full_cols.difference(test_cols)\n",
    "train_not_test = train_cols.difference(test_cols)\n",
    "test_not_train = test_cols.difference(train_cols)\n",
    "train_and_test = train_cols.intersection(test_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping missing values in the datasets\n",
    "full.dropna(how='any', inplace=True, axis=0)\n",
    "test.dropna(how='any', inplace=True, axis=0)\n",
    "train.dropna(how='any', inplace=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting date to datetime\n",
    "full.date = pd.to_datetime(full.date)\n",
    "train.date = pd.to_datetime(train.date)\n",
    "test.date = pd.to_datetime(test.date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the data and inspected it. We addressed the missing values. Since recovery is an important target, we checked whether it was calculated correctly. Our calculated recovery was then compared with the recovery value in the data. The difference between the two values was no more than a rounding error, with an MAE of 10.4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a filter for desired columns\n",
    "au_cols = ['rougher.input.feed_au', 'rougher.output.tail_au', 'rougher.output.concentrate_au', 'primary_cleaner.output.tail_au', \n",
    "        'primary_cleaner.output.concentrate_au', 'secondary_cleaner.output.tail_au', 'final.output.concentrate_au', 'final.output.tail_au']\n",
    "\n",
    "ag_cols = ['rougher.input.feed_ag', 'rougher.output.tail_ag', 'rougher.output.concentrate_ag', 'primary_cleaner.output.tail_ag', \n",
    "        'primary_cleaner.output.concentrate_ag', 'secondary_cleaner.output.tail_ag','final.output.concentrate_ag', 'final.output.tail_ag'] \n",
    "\n",
    "pb_cols = ['rougher.input.feed_pb', 'rougher.output.tail_pb', 'rougher.output.concentrate_pb', 'primary_cleaner.output.tail_pb', \n",
    "        'primary_cleaner.output.concentrate_pb', 'secondary_cleaner.output.tail_pb','final.output.concentrate_pb', 'final.output.tail_pb'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering for different metals \n",
    "au = full[au_cols]\n",
    "ag = full[ag_cols]\n",
    "pb = full[pb_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of gold columns\n",
    "au.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for recovery of gold\n",
    "def au_recovery(data):\n",
    "    c = data['rougher.output.concentrate_au'] \n",
    "    f = data['rougher.input.feed_au']\n",
    "    t = data['rougher.output.tail_au']\n",
    "    flot_recovery = (c * (f-t)) / (f * (c-t)) * 100\n",
    "    return print(f'{flot_recovery.sum():,}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold recovery\n",
    "au_recovery(au)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating au recovery variable\n",
    "au_recovery_val = 1344794.762862905"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot of gold concentration\n",
    "px.bar(au.sum(), title='Concentration of Gold', color=['concentrate','tail', 'concentrate', 'tail', 'concentrate', 'tail', 'concentrate', 'tail'], \n",
    "    log_y=True, height=900)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The recovery of gold increases throughout the purification process, from the rougher input to the final output concentrate. The tail output of the process sees the highest concentration after the primary and secondary cleaning phases. The final output tail then sees some gold. This is intuitive, as the company goal is to extract and purify gold from gold ore. The final output of gold should be higher, especially after multiple rounds of purification. Also, we would see the most loss, with gold in the tails, during the cleaning process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of silver columns\n",
    "ag.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for the silver recovery \n",
    "def ag_recovery(data):\n",
    "    c = data['rougher.output.concentrate_ag'] \n",
    "    f = data['rougher.input.feed_ag']\n",
    "    t = data['rougher.output.tail_ag']\n",
    "    flot_recovery = (c * (f-t)) / (f * (c-t)) * 100\n",
    "    return print(f'{flot_recovery.sum():,}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silver recovery\n",
    "ag_recovery(ag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating silver recovery variable\n",
    "ag_recovery_val = 1008724.5136078214"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot of silver concentration\n",
    "px.bar(ag.sum(), title='Concentration of Silver', color=['concentrate','tail', 'concentrate', 'tail', 'concentrate', 'tail', 'concentrate', 'tail'], \n",
    "    log_y=True, height=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see silver is heavily extracted in the rougher output, and the amount decreases throughout the process until the final output, which is comparatively lower than the rougher input. This is due to most of the silver being lost in the tail. The primary and secondary cleaning steps remove the largest amount of silver from the process, leaving roughly half of the silver from those steps at the final tail output. This is in line with logic, as silver is a byproduct of the process. Since our target is gold, it makes sense that most of the silver would be in the tail at the end of the process. ALso note the scale difference in concentration between gold and silver. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of lead columns\n",
    "pb.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for lead recovery\n",
    "def pb_recovery(data):\n",
    "    c = data['rougher.output.concentrate_pb'] \n",
    "    f = data['rougher.input.feed_pb']\n",
    "    t = data['rougher.output.tail_pb']\n",
    "    flot_recovery = (c * (f-t)) / (f * (c-t)) * 100\n",
    "    return print(f'{flot_recovery.sum():,}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lead recovery\n",
    "pb_recovery(pb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lead recovery variable\n",
    "pb_recovery_val = 1389343.5783014493"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot of lead concentration\n",
    "px.bar(pb.sum(), title='Concentration of Lead', color=['concentrate','tail', 'concentrate', 'tail', 'concentrate', 'tail', 'concentrate', 'tail'], \n",
    "    log_y=True, height=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lead is recovered at a much lower concentration than the target gold, and the byproduct of silver. While the aforementioned are precious metals with similar chemical properties, lead is markedly different. Relative to the amount in the rougher input, we see roughly triple the initial amount in the final concentrate. The cleaning processes remove the most lead, leaving a concentration in the final tail output that is similar to the rougher initial feed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Metal Concentrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure comparing metal concentrations\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(x=au.columns, y=au.sum(), name='gold', marker_color = 'gold'))\n",
    "\n",
    "fig.add_trace(go.Bar(x=ag.columns, y=ag.sum(), name='silver', marker_color='silver'))\n",
    "\n",
    "fig.add_trace(go.Bar(x=pb.columns, y=pb.sum(), name='lead', marker_color='black'))\n",
    "\n",
    "fig.update_layout(barmode='group', height=900, title='Change in Metal Concentration')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since gold is the desired product, it is intuitive to see the most gold in the final output concentrate, and very little in the output tail. More gold is present in concentrate than silver and lead. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metal Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a recovery dataframe with metal recovery values\n",
    "recovery_df = pd.DataFrame({'metal': ['au', 'ag', 'pb'], 'values': [au_recovery_val, ag_recovery_val, pb_recovery_val]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# bar plot of recovery dataframe\n",
    "px.bar(recovery_df, y='values', x='metal', color='metal', title='Metal Recovery')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are comparing the total recovered of each metal. Since more gold is in the final concentrate than in the tail, the recovery is high. The same applies to lead. Conversely, more silver is found in the tail compared to the concentrate, resulting in lower recovery. This is good, because we would not want to have too much silver in the concentrate, as its chemical properties are similar to gold. One of those key properties is melting point, which would make separating these two elements more difficult. The melting point of lead is far different from gold, which would make separation easier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Feed Particle Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating train and test sets\n",
    "feed_train = train[['primary_cleaner.input.feed_size', 'rougher.input.feed_size']]\n",
    "feed_test = test[['primary_cleaner.input.feed_size', 'rougher.input.feed_size']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing train and test set average particle size\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(x=feed_train.columns, y=feed_train.mean(), name='train', marker_color = 'black'))\n",
    "\n",
    "fig.add_trace(go.Bar(x=feed_test.columns, y=feed_test.mean(), name='test', marker_color='blue'))\n",
    "\n",
    "fig.update_layout(barmode='group', height=900, title='Feed Particle Size')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of feed train\n",
    "px.histogram(feed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of feed test\n",
    "px.histogram(feed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dstributions of feed particle sizes\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Histogram(x=feed_train['primary_cleaner.input.feed_size'], name='primary train', marker_color = 'black'))\n",
    "fig.add_trace(go.Histogram(x=feed_train['rougher.input.feed_size'], name='rougher train', marker_color = 'blue'))\n",
    "fig.add_trace(go.Histogram(x=feed_test['primary_cleaner.input.feed_size'], name='primary test', marker_color='green'))\n",
    "fig.add_trace(go.Histogram(x=feed_test['rougher.input.feed_size'], name='rougher test', marker_color='yellow'))\n",
    "fig.update_layout(height=900, title='Feed Particle Size')\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph illustrates the particle size of the feed decreasing throughout the process. This is crucial, as the particle size is influential in the recovery of gold in ore. Gold dissolution increases with decreasing particle size. Consequently, the distribution of particle size in the training and test set needs to be similar so that the model will evaluate correctly. We see that the test and train samples have a similar distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concentration values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making filters for concentration\n",
    "au_conc = ['rougher.output.concentrate_au', 'primary_cleaner.output.concentrate_au', 'final.output.concentrate_au']\n",
    "\n",
    "ag_conc = ['rougher.output.concentrate_ag', 'primary_cleaner.output.concentrate_ag', 'final.output.concentrate_ag']\n",
    "\n",
    "pb_conc = ['rougher.output.concentrate_pb', 'primary_cleaner.output.concentrate_pb', 'final.output.concentrate_pb']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering full dat set for gold concentrations\n",
    "full[au_conc].value_counts(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values of rougher output concentrations for gold\n",
    "full['rougher.output.concentrate_au'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary stats on gold rougher output concentrate\n",
    "full['rougher.output.concentrate_au'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of gold concentrate\n",
    "px.histogram(full['rougher.output.concentrate_au'], title='Gold Concentrate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold concentration\n",
    "px.bar(full[au_conc].sum(), color=['Flotation', 'Primary Cleaner', 'Secondary Cleaner'], title='Concentration of Gold', log_y=True, height=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rougher output of gold appears to be normally distributed around the mean of 20.4. We see outliers in our data, where the output concentrate is 0 for 301 samples. Overall, the trend is an increase in gold concentration throughout the various processes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values of silver concentrate\n",
    "full['rougher.output.concentrate_ag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary stats on silver concentrate\n",
    "full['rougher.output.concentrate_ag'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of silver concentrate\n",
    "px.histogram(full['rougher.output.concentrate_ag'], title='Silver Concentrate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concentration of silver\n",
    "px.bar(full[ag_conc].sum(), color=['Flotation', 'Primary Cleaner', 'Secondary Cleaner'], title='Concentration of Silver', log_y=True, height=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silver concentration appears to be normally distributed around the mean of 12.3, also with 301 rougher values of 0. The concentration of silver decreases throughout the process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values of lead concentration\n",
    "full['rougher.output.concentrate_pb'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary stats on lead concentration\n",
    "full['rougher.output.concentrate_pb'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of lead concentration\n",
    "px.histogram(full['rougher.output.concentrate_pb'], title='Lead Concentrate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lead concentration\n",
    "px.bar(full[pb_conc].sum(), color=['Flotation', 'Primary Cleaner', 'Secondary Cleaner'], title='Concentration of Lead', log_y=True, height=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concentration of lead appears somewhat normally distributed around the mean of 7.7. There are 301 values with 0 rougher output concentrations. The Concentration of lead increases throughout the process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of metals at different stages\n",
    "rougher_input = full[['rougher.input.feed_au','rougher.input.feed_ag', 'rougher.input.feed_pb']].sum(axis=1)\n",
    "\n",
    "rougher_output = full[['rougher.output.tail_au','rougher.output.tail_ag', 'rougher.output.tail_pb']].sum(axis=1)\n",
    "\n",
    "rougher_concentrate = full[['rougher.output.concentrate_au','rougher.output.concentrate_ag', \n",
    "                            'rougher.output.concentrate_pb']].sum(axis=1)\n",
    "\n",
    "cleaner_output = full[['primary_cleaner.output.tail_au', 'primary_cleaner.output.tail_ag', \n",
    "                       'primary_cleaner.output.tail_pb' ]].sum(axis=1)\n",
    "\n",
    "cleaner_concentrate = full[['primary_cleaner.output.concentrate_au', 'primary_cleaner.output.concentrate_ag', \n",
    "                            'primary_cleaner.output.concentrate_pb']].sum(axis=1)\n",
    "\n",
    "secondary_output = full[['secondary_cleaner.output.tail_au', 'secondary_cleaner.output.tail_ag', \n",
    "                         'secondary_cleaner.output.tail_pb' ]].sum(axis=1)\n",
    "\n",
    "final_tail = full[['final.output.tail_au', 'final.output.tail_ag', 'final.output.tail_pb']].sum(axis=1)\n",
    "\n",
    "final_concentrate = full[['final.output.concentrate_au', 'final.output.concentrate_ag', \n",
    "                          'final.output.concentrate_pb']].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of metals at various stages\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Histogram(x=rougher_input, name='Rougher Input Feed', marker_color = 'black'))\n",
    "fig.add_trace(go.Histogram(x=rougher_output, name='Rougher Output Tail', marker_color = 'blue'))\n",
    "fig.add_trace(go.Histogram(x=rougher_concentrate, name='Rougher Output Concentrate', marker_color='green'))\n",
    "fig.add_trace(go.Histogram(x=cleaner_output, name='Primary Cleaner Output Tail', marker_color='pink'))\n",
    "fig.add_trace(go.Histogram(x=cleaner_concentrate, name='Primary Cleaner Output Concentrate', marker_color = 'red'))\n",
    "fig.add_trace(go.Histogram(x=secondary_output, name='Secondary Cleaner Output Tail', marker_color='orange'))\n",
    "fig.add_trace(go.Histogram(x=final_tail, name='Final Output Tail', marker_color='purple'))\n",
    "fig.add_trace(go.Histogram(x=final_concentrate, name='Final Output Concentrate', marker_color='yellow'))\n",
    "\n",
    "fig.update_layout(height=900, title='Purification Stages')\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data will need to be cleaned of the 0, and near zero concentration sums shown on the lower left of the distribution. Input feeds of near zero have no value. The other values in this area of the distribution should be removed as well. These values are anomalies, as they defy the law of conservation of mass. If the input of the process is not zero, then the outputs should not be zero. What is not found in the concentrate should be found in the tail, and vice versa. These zero values represent ore that has disappeared from the system. The data also illustrates gold concentration increasing throughout the extraction and purification processes. Lead also increases in concentration, but at a much smaller scale to gold. Silver concentration decreases throughout the process, as most of the silver is removed to the tails.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the count rows\n",
    "full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the zero concentration values from the datasets \n",
    "full = full[full['rougher.output.concentrate_ag'] > 0.25]\n",
    "train = train[train['rougher.output.concentrate_ag'] > 0.25]\n",
    "\n",
    "\n",
    "full = full[full['rougher.output.concentrate_au'] > 0.25]\n",
    "train = train[train['rougher.output.concentrate_au'] > 0.25]\n",
    "\n",
    "\n",
    "full = full[full['rougher.output.concentrate_pb'] > 0.25]\n",
    "train = train[train['rougher.output.concentrate_pb'] > 0.25]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensuring the number of rows has changed to account for the removal of zero concentration values\n",
    "full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the shape of train dataset\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the shape of test dataset\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take dates from data\n",
    "date = full['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of rows matches full shape\n",
    "date.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging full with date, as key for merging\n",
    "full_date = pd.concat([date, full[full_not_test]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of the dataset\n",
    "full_date.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging full with test, to incorporate missing columns\n",
    "full_test = test.merge(full_date, left_on='date', right_on='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check columns \n",
    "full_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensuring we have a total of 87 columns\n",
    "full_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the differences, and similarities in the columns of the datasets\n",
    "full_not_test = full_cols.difference(test_cols)\n",
    "train_not_test = train_cols.difference(test_cols)\n",
    "test_not_train = test_cols.difference(train_cols)\n",
    "train_and_test = train_cols.intersection(test_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these columns are missing from the test dataset, but are in the training data\n",
    "train_not_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When making features, we have to limit the training set to features we have in common with the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the features and training samples from the datasets\n",
    "features_train = train[train_and_test].drop(['date'], axis=1)\n",
    "target_train = train[['final.output.recovery' , 'rougher.output.recovery']]\n",
    "\n",
    "features_test = full_test[train_and_test].drop(['date'], axis=1)\n",
    "target_test = full_test[['final.output.recovery', 'rougher.output.recovery']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function to calculate sMAPE.\n",
    "def smape(y_true, y_pred):\n",
    "    smape = 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "    return smape\n",
    "\n",
    "#Create function to calculate final sMAPE.\n",
    "def f_smape(y_true, y_pred):\n",
    "    predicted_rough, predicted_final = y_pred[:, 1], y_pred[:, 0]\n",
    "    true_rough, true_final = y_true.iloc[:, 1], y_true.iloc[:, 0]\n",
    "    f_smape = (.25 * (smape(true_rough, predicted_rough))) + (.75 * (smape(true_final, predicted_final)))\n",
    "    return f_smape\n",
    "\n",
    "#Create function to calculate final sMAPE.\n",
    "def f_smape2(y_true, y_pred):\n",
    "    predicted_rough, predicted_final = y_pred[:, 1], y_pred[:, 0]\n",
    "    true_rough, true_final = y_true.iloc[:, 1], y_true.iloc[:, 0]\n",
    "    f_smape = -1 * (.25 * (smape(true_rough, predicted_rough))) + (.75 * (smape(true_final, predicted_final)))\n",
    "    return f_smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning our smape function into a scorer for cross validation \n",
    "f_smape_score = make_scorer(f_smape2, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "model1 = DecisionTreeRegressor(random_state=19)\n",
    "model1.fit(features_train, target_train) # train model on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation using final smape as scoring\n",
    "scores1 = cross_val_score(model1, features_train, target_train, scoring=f_smape_score, cv=5) \n",
    "final_score1 = sum(scores1) / len(scores1)\n",
    "print('Average model evaluation score:', final_score1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest \n",
    "model2 = RandomForestRegressor(random_state=19)\n",
    "model2.fit(features_train, target_train) # train model on training set\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation using final smape as scoring\n",
    "scores2 = cross_val_score(model2, features_train, target_train, scoring=f_smape_score, cv=5) \n",
    "final_score2 = sum(scores2) / len(scores2)\n",
    "print('Average model evaluation score:', final_score2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression\n",
    "model3 = LinearRegression() # initialize model constructor\n",
    "model3.fit(features_train, target_train) # train model on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cross validation using final smape as scoring\n",
    "scores3 = cross_val_score(model3, features_train, target_train, scoring=f_smape_score, cv=5) \n",
    "final_score3 = sum(scores3) / len(scores3)\n",
    "print('Average model evaluation score:', final_score3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# final model \n",
    "final_model = RandomForestRegressor(random_state=19)\n",
    "final_model.fit(features_train, target_train)\n",
    "final_predictions = final_model.predict(features_test)\n",
    "result = f_smape(target_test, final_predictions)\n",
    "print('Final sMAPE score of test data: ', result) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dummy regressor to mimic a constant model that always predicts mean of the train set targets\n",
    "dummy_regr = DummyRegressor(strategy='mean')\n",
    "dummy_regr.fit(features_train, target_train)\n",
    "dummy_predictions = dummy_regr.predict(features_test)\n",
    "f_smape(target_test, dummy_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we negated and maximized our scoring function, the model with the highest sMAPE is the best model, when comparing cross validation scores. The best model to use is a decision tree regressor. We create a final model and achieve a sMAPE score of 11.18. This compares to a dummy model, where the model is always predicting the mean of train set targets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, we were able to work with the data we received to complete the project. We ensured the recovery column was calculated correctly, by comparing it with our calculated values. We looked at the distribution of concentrations for the various metals, and saw anomalies, which where removed. The data illustrated the increase in gold concentrations in the final product, and a small amount of gold in the tails. We also looked at the recovery of gold, and compared it with the other metals. Finally, we successfully trained a model that could predict the gold recovery, and we found the decision tree to be the best model to use. Therefore, Zyfra can use this model to optimize their gold ore refining process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "4f2b0cfe12c109b58467a02dc33230d9e6228c23b43020d2941c37e2a7dfdd3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
